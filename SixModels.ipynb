{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8927fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895cb987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d13ad4c",
   "metadata": {},
   "source": [
    "# Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97709ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that cleans the dataset represented by a Pandas dataframe passed into it \n",
    "def clean_dataset(dataset):\n",
    "    # With combining various datasets, there are bound to be duplicates, and this removes such duplicates if they exist\n",
    "    duplicate_rows_data = dataset[dataset.duplicated()]\n",
    "    dataset = dataset.drop_duplicates()\n",
    "    #Remove all rows that do not have 0 or 1 as the label\n",
    "    dataset = dataset[(dataset.label == 0) | (dataset.label == 1)]\n",
    "    dataset.isnull().sum()\n",
    "    #Remove any rows that have null in any of the columns \n",
    "    dataset = dataset.dropna(how='any',axis=0) \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9108e63",
   "metadata": {},
   "source": [
    "Dataset 1 (IEEE DataPort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a39905",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_data = pd.read_csv('Data/IEEE/ieee_true.csv')\n",
    "true_data = true_data.drop('Label', 1)\n",
    "true_data = true_data.drop('Publisher', 1)\n",
    "true_data = true_data.drop('Username', 1)\n",
    "true_data = true_data.drop('Region', 1)\n",
    "true_data = true_data.drop('Link', 1)\n",
    "true_data = true_data.drop('Date Posted', 1)\n",
    "#Drop the column 'Label' and replace it with 'label'\n",
    "true_data['label']=[1]*len(true_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f043ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = pd.read_csv('Data/IEEE/ieee_fake.csv')\n",
    "fake_data = fake_data.drop('Binary Label', 1)\n",
    "fake_data = fake_data.drop('Poynter_Label', 1)\n",
    "fake_data = fake_data.drop('Fact_checked_by', 1)\n",
    "fake_data = fake_data.drop('Origin_URL', 1)\n",
    "fake_data = fake_data.drop('Origin', 1)\n",
    "fake_data = fake_data.drop('Explanation', 1)\n",
    "fake_data = fake_data.drop('Country', 1)\n",
    "fake_data = fake_data.drop('Region', 1)\n",
    "fake_data = fake_data.drop('Link', 1)\n",
    "fake_data = fake_data.drop('Date Posted', 1)\n",
    "fake_data['label']=[0]*len(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d66b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append the true and fake datasets\n",
    "dataset_1=true_data.append(fake_data).sample(frac=1).reset_index()\n",
    "dataset_1 = dataset_1.drop('index', 1)\n",
    "dataset_1 = clean_dataset(dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c5951",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbb4fd9",
   "metadata": {},
   "source": [
    "Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12762035",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = pd.read_csv('Data/zenodo_dataset.csv')\n",
    "#Rename the hedalines and outcome columns to the names we are looking for \n",
    "dataset_2 = dataset_2.rename(columns={'headlines':'Text'})\n",
    "dataset_2 = dataset_2.rename(columns={'outcome':'label'})\n",
    "dataset_2 = clean_dataset(dataset_2)\n",
    "dataset_2.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb25639",
   "metadata": {},
   "source": [
    "Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6cb40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diptamath_first = pd.read_csv('Data/Diptamath/first_set.csv')\n",
    "diptamath_first = diptamath_first.drop('id', 1)\n",
    "diptamath_first = diptamath_first.rename(columns={'tweet':'Text'})\n",
    "#Replaces 'fake' with 0, 1 otherwise\n",
    "diptamath_first.loc[diptamath_first['label'] == 'fake', 'label'] = 0\n",
    "diptamath_first.loc[diptamath_first['label'] == 'real', 'label'] = 1\n",
    "diptamath_first.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4908b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "diptamath_second = pd.read_csv('Data/Diptamath/second_set.csv')\n",
    "diptamath_second = diptamath_second.drop('id', 1)\n",
    "diptamath_second = diptamath_second.rename(columns={'tweet':'Text'})\n",
    "diptamath_second.loc[diptamath_second['label'] == 'fake', 'label'] = 0\n",
    "diptamath_second.loc[diptamath_second['label'] == 'real', 'label'] = 1\n",
    "diptamath_second.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e04e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3=diptamath_first.append(diptamath_second).sample(frac=1).reset_index()\n",
    "dataset_3 = dataset_3.drop('index', 1)\n",
    "dataset_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f901b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b33ce2",
   "metadata": {},
   "source": [
    "Dataset 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c7e6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_4 = pd.read_csv('Data/github_dataset.csv')\n",
    "dataset_4 = dataset_4.rename(columns={'News':'Text'})\n",
    "dataset_4 = dataset_4.rename(columns={'Outcome':'label'})\n",
    "dataset_4 = clean_dataset(dataset_4)\n",
    "dataset_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1dac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_4.loc[dataset_4['label'] == 'fake', 'label'] = 0\n",
    "dataset_4.loc[dataset_4['label'] == 'real', 'label'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d514c",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove whitespace from text\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee421822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In some cases, there is no space after a period, comma, or dash in an article, and as a result, when the special characters are removed \n",
    "#the last word of the previous sentence and first word in the next sentence are added together \n",
    "\n",
    "def add_space_after_period(text):\n",
    "    text = re.sub(r'(?<=[.,-])(?=[^\\s])', r' ', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b62c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove any emails \n",
    "def remove_emails(text):\n",
    "    return re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove any HTML tags\n",
    "def remove_html_tags(text):\n",
    "    parser = BeautifulSoup(text, \"html.parser\")\n",
    "    without_html = parser.get_text(separator = \" \")\n",
    "    return without_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c456fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rt(text):\n",
    "    return re.sub(r'\\brt\\b', '', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove any special characters\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub(r'[^\\w ]+', \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a5a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return ' '.join([t for t in text.split() if t not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d108315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upon inspecting the cleaned data, there were several tokens of the form pictwittercom... that needed to be removed \n",
    "def remove_twitter_pics(text):\n",
    "    text = re.sub(r\"pictwittercom.*\", '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dec7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the spelling from British to American\n",
    "def convert_to_american(text):\n",
    "    # Copied from here\n",
    "    text = re.sub(r\"(...)our$\", r\"\\1or\", text)\n",
    "    text = re.sub(r\"([bt])re$\", r\"\\1er\", text)\n",
    "    text = re.sub(r\"([iy])s(e$|ing|ation)\", r\"\\1z\\2\", text)\n",
    "    text = re.sub(r\"ogue$\", \"og\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a84e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    result=[]\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    for token,tag in pos_tag(text):\n",
    "        pos=tag[0].lower()\n",
    "        \n",
    "        if pos not in ['a', 'r', 'n', 'v']:\n",
    "            pos='n'\n",
    "            \n",
    "        result.append(wordnet.lemmatize(token,pos))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ddecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    clean = remove_whitespace(text)\n",
    "    clean = clean.strip()\n",
    "    clean = clean.lower()\n",
    "    clean = remove_emails(clean)\n",
    "    clean = remove_html_tags(clean)\n",
    "    clean = remove_rt(clean)\n",
    "    clean = add_space_after_period(clean)\n",
    "    clean = remove_special_characters(clean)\n",
    "    clean = remove_accented_chars(clean)\n",
    "    clean = remove_twitter_pics(clean)\n",
    "    clean = convert_to_american(clean)\n",
    "    clean = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", clean)\n",
    "    clean = word_tokenize(clean)\n",
    "    clean = lemmatization(clean)\n",
    "    return clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c0371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1['Text'] = dataset_1['Text'].apply(lambda x: clean_text(x))\n",
    "dataset_1['Text'] = dataset_1['Text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "dataset_2['Text'] = dataset_2['Text'].apply(lambda x: clean_text(x))\n",
    "dataset_2['Text'] = dataset_2['Text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "dataset_3['Text'] = dataset_3['Text'].apply(lambda x: clean_text(x))\n",
    "dataset_3['Text'] = dataset_3['Text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "dataset_4['Text'] = dataset_4['Text'].apply(lambda x: clean_text(x))\n",
    "dataset_4['Text'] = dataset_4['Text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f8c8c",
   "metadata": {},
   "source": [
    "# Running Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40609839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "from sklearn import svm\n",
    "import xlsxwriter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_model(data):\n",
    "    data.label = data.label.astype('int')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['Text'], data.label, test_size=0.20, random_state=42, shuffle=True)\n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "def feature_extractor(X_train, X_test, y_train, y_test, data):\n",
    "    y_train=y_train.astype('int')\n",
    "    y_test=y_test.astype('int')\n",
    "    #Initialize the `tfidf_vectorizer` \n",
    "    tfidf_vectorizer_ngram = TfidfVectorizer(analyzer= 'word', stop_words='english', ngram_range=(1, 2), min_df=2)\n",
    "    #Fit and transform the training data \n",
    "    tfidf_train_ngram = tfidf_vectorizer_ngram.fit_transform(X_train)\n",
    "    #Transform the test set \n",
    "    tfidf_test_ngram = tfidf_vectorizer_ngram.transform(X_test)\n",
    "    return tfidf_vectorizer_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cfd52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(X_train, X_test, y_train, y_test, tfidf_vectorizer_ngram):    \n",
    "    #Multinomial Naive Bayes\n",
    "    nb_pipeline_ngram = Pipeline([\n",
    "        ('nb_tfidf',tfidf_vectorizer_ngram),\n",
    "        ('nb_clf',MultinomialNB())])\n",
    "    nb_pipeline_ngram.fit(X_train,y_train)\n",
    "    predicted_nb_ngram = nb_pipeline_ngram.predict(X_test)\n",
    "    accuracy = np.mean(predicted_nb_ngram == y_test)\n",
    "    print(\"Multinomal Naive Bayes:\", accuracy)\n",
    "    \n",
    "    param_grid = {'nb_tfidf__ngram_range': [(1,2), (1,3)],\n",
    "                  'nb_clf__alpha': [0.1, 1.0],\n",
    "                  'nb_clf__fit_prior': [True, False]}\n",
    "    \n",
    "    grid = GridSearchCV(nb_pipeline_ngram, param_grid, scoring='accuracy') \n",
    "    grid.fit(X_train, y_train) \n",
    "\n",
    "    # print best parameter after tuning \n",
    "    print(grid.best_params_) \n",
    "    grid_predictions = grid.predict(X_test) \n",
    "    accuracy = np.mean(grid_predictions == y_test)\n",
    "    print(\"Multinomal Naive Bayes:\", accuracy)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #building classifier using logistic regression\n",
    "    logR_pipeline_ngram = Pipeline([\n",
    "            ('LogRCV_tfidf_ngram',tfidf_vectorizer_ngram),\n",
    "            ('LogR_clf',LogisticRegression(max_iter=10000))\n",
    "            ])\n",
    "    logR_pipeline_ngram.fit(X_train,y_train)\n",
    "    predicted_LogR_ngram = logR_pipeline_ngram.predict(X_test)\n",
    "    accuracy = np.mean(predicted_LogR_ngram == y_test)\n",
    "    print(\"Logistic Regression:\", accuracy)\n",
    "    \n",
    "    \n",
    "    param_grid = {'LogR_clf__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                 'LogR_clf__C': [100, 10],\n",
    "                 'LogR_clf__class_weight': [None, 'balanced'],\n",
    "                 'LogR_clf__multi_class': ['auto', 'ovr'],\n",
    "                  'LogR_clf__tol': [0.01, 0.1]}\n",
    "\n",
    "    grid = GridSearchCV(logR_pipeline_ngram, param_grid, scoring='accuracy', verbose=True) \n",
    "    grid.fit(X_train, y_train) \n",
    "\n",
    "    # print best parameter after tuning \n",
    "    print(grid.best_params_) \n",
    "    grid_predictions = grid.predict(X_test) \n",
    "    accuracy = np.mean(grid_predictions == y_test)\n",
    "    print(\"Logistic Regression:\", accuracy)\n",
    "    \n",
    "    \n",
    "    #building Linear SVM classfier\n",
    "    svm_pipeline_ngram = Pipeline([\n",
    "            ('svm_tfidf',tfidf_vectorizer_ngram),\n",
    "            ('svm_clf',svm.LinearSVC(max_iter=100000))\n",
    "            ])\n",
    "    svm_pipeline_ngram.fit(X_train,y_train)\n",
    "    predicted_svm_ngram = svm_pipeline_ngram.predict(X_test)\n",
    "    accuracy = np.mean(predicted_svm_ngram == y_test)\n",
    "    print(\"Linear SVM Classifier:\", accuracy)\n",
    "    \n",
    "    param_grid = {'svm_tfidf__ngram_range': [(1, 1), (1, 2),(1,3)],\n",
    "                  'svm_clf__tol': [0.0001, 0.01, 0.1],\n",
    "                  'svm_clf__loss': ['hinge', 'squared_hinge'],\n",
    "                  'svm_clf__class_weight': [None, 'balanced']\n",
    "                 }\n",
    "\n",
    "    grid = GridSearchCV(svm_pipeline_ngram, param_grid) \n",
    "    grid.fit(X_train, y_train) \n",
    "\n",
    "    # print best parameter after tuning \n",
    "    print(grid.best_params_) \n",
    "    grid_predictions = grid.predict(X_test) \n",
    "    accuracy = np.mean(grid_predictions == y_test)\n",
    "    print(\"Linear SVM Classifier:\", accuracy)\n",
    "    \n",
    "    \n",
    "    #building Passive Aggressive Classifier\n",
    "    passive_pipeline_ngram = Pipeline([\n",
    "            ('pass_tfidf',tfidf_vectorizer_ngram),\n",
    "            ('pas_clf',PassiveAggressiveClassifier())\n",
    "            ])\n",
    "    passive_pipeline_ngram.fit(X_train,y_train)\n",
    "    predicted_passive_ngram = passive_pipeline_ngram.predict(X_test)\n",
    "    accuracy = np.mean(predicted_passive_ngram == y_test)\n",
    "    print(\"Passive Aggressive Classifier:\", accuracy)\n",
    "    \n",
    "    \n",
    "    param_grid = {'pass_tfidf__ngram_range': [(1,1), (1, 2),(1,3)],\n",
    "                  'pas_clf__C': [1, 10],\n",
    "                  'pas_clf__fit_intercept': [True, False], \n",
    "                  'pas_clf__tol': [0.0001, 0.01, 0.1],\n",
    "                  'pas_clf__early_stopping': [True, False]}\n",
    "    grid = GridSearchCV(passive_pipeline_ngram, param_grid) \n",
    "    grid.fit(X_train, y_train) \n",
    "\n",
    "    # print best parameter after tuning \n",
    "    print(grid.best_params_) \n",
    "    grid_predictions = grid.predict(X_test) \n",
    "    accuracy = np.mean(grid_predictions == y_test)\n",
    "    print(\"Passive Aggressive Classifier:\", accuracy)\n",
    "    \n",
    "    \n",
    "    #building Decision Tree Classifier\n",
    "    decision_pipeline_ngram = Pipeline([\n",
    "            ('decision_tfidf_ngram',tfidf_vectorizer_ngram),\n",
    "            ('decision_clf',DecisionTreeClassifier())\n",
    "            ])\n",
    "    decision_pipeline_ngram.fit(X_train,y_train)\n",
    "    predicted_decision_ngram = decision_pipeline_ngram.predict(X_test)\n",
    "    accuracy = np.mean(predicted_decision_ngram == y_test)\n",
    "    print(\"Decision Tree Classifier:\", accuracy)\n",
    "    \n",
    "    param_grid = {'decision_tfidf_ngram__ngram_range': [(1,2), (1,3)],\n",
    "                  'decision_clf__criterion': [\"gini\", \"entropy\"],\n",
    "                  'decision_clf__splitter': [\"best\", \"random\"],\n",
    "                  'decision_clf__max_depth': [30],\n",
    "                  'decision_clf__min_samples_split': [0.5, 1.0, 2], \n",
    "                  'decision_clf__class_weight': [None, 'balanced']}\n",
    "\n",
    "\n",
    "    grid = GridSearchCV(decision_pipeline_ngram, param_grid) \n",
    "    grid.fit(X_train, y_train) \n",
    "\n",
    "    # print best parameter after tuning \n",
    "    print(grid.best_params_) \n",
    "    grid_predictions = grid.predict(X_test) \n",
    "    accuracy = np.mean(grid_predictions == y_test)\n",
    "    print(\"Decision Tree Classifier:\", accuracy)\n",
    "    \n",
    "    #building Random Forest Classifier\n",
    "    forest_pipeline_ngram = Pipeline([\n",
    "            ('forest_tfidf_ngram',tfidf_vectorizer_ngram),\n",
    "            ('forest_clf',RandomForestClassifier(n_estimators=50))\n",
    "            ])\n",
    "    forest_pipeline_ngram.fit(X_train,y_train)\n",
    "    predicted_forest_ngram = forest_pipeline_ngram.predict(X_test)\n",
    "    accuracy = np.mean(predicted_forest_ngram == y_test)\n",
    "    print(\"Random Forest Classifier:\", accuracy)\n",
    "    \n",
    "    param_grid = {'forest_clf__criterion': [\"gini\", \"entropy\"],\n",
    "                  'forest_clf__class_weight': [None, \"balanced\", \"balanced_subsample\"],\n",
    "                  'forest_clf__bootstrap': [True, False]}\n",
    "    grid = GridSearchCV(forest_pipeline_ngram, param_grid) \n",
    "    grid.fit(X_train, y_train) \n",
    "\n",
    "    # print best parameter after tuning \n",
    "    print(grid.best_params_) \n",
    "    grid_predictions = grid.predict(X_test) \n",
    "    accuracy = np.mean(grid_predictions == y_test)\n",
    "    print(\"Decision Tree Classifier:\", accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f5c207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = break_model(dataset_1)\n",
    "dataset1_extractor = feature_extractor(X_train, X_test, y_train, y_test, dataset_1)\n",
    "run_models(X_train, X_test, y_train, y_test, dataset1_extractor)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = break_model(dataset_2)\n",
    "dataset2_extractor = feature_extractor(X_train, X_test, y_train, y_test, dataset_2)\n",
    "run_models(X_train, X_test, y_train, y_test, dataset2_extractor)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = break_model(dataset_3)\n",
    "dataset3_extractor = feature_extractor(X_train, X_test, y_train, y_test, dataset_3)\n",
    "run_models(X_train, X_test, y_train, y_test, dataset3_extractor)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = break_model(dataset_4)\n",
    "dataset4_extractor = feature_extractor(X_train, X_test, y_train, y_test, dataset_4)\n",
    "run_models(X_train, X_test, y_train, y_test, dataset4_extractor)\n",
    "print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
